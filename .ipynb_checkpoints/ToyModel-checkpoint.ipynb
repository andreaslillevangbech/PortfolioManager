{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.keras.Input(shape = (3,), batch_size=100)\n",
    "x = tf.keras.layers.Dense(5, activation='relu')(inputs)\n",
    "\n",
    "\n",
    "\n",
    "x = np.random.rand(batch_size, 1, 3)\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(b)\n",
    "    y = model(x)\n",
    "\n",
    "print([var.name for var in tape.watched_variables()])\n",
    "grads = tape.gradient(y, [b, model.trainable_variables])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cash(tf.keras.Model):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Cash, self).__init__()\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.b = tf.Variable(tf.zeros((1, 1), dtype=tf.float32), name='b', trainable=True)\n",
    "        #self.b = self.add_weight(shape=(1,), initializer='zeros', dtype=tf.float32, name='b')\n",
    "    \n",
    "    def call(self, x):\n",
    "        print(x.shape)\n",
    "        b = tf.tile(self.b, [x.shape[0], 1])\n",
    " \n",
    "        \n",
    "        x = tf.concat((b, x), axis = -1)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "model = Cash()\n",
    "    \n",
    "X = np.random.randn(100, 3)\n",
    "model.predict(X)\n",
    "\n",
    "\"\"\"\n",
    "with tf.GradientTape() as tape:\n",
    "    y = model(X)\n",
    "\n",
    "print([var.name for var in tape.watched_variables()])\n",
    "grads = tape.gradient(y, model.trainable_variables)\n",
    "grads\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['b:0', 'dense_11/kernel:0', 'dense_11/bias:0']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[5.50472e-09]], dtype=float32)>,\n",
       " [<tf.Tensor: shape=(3, 5), dtype=float32, numpy=\n",
       "  array([[ 0.0000000e+00,  2.1478998e-08,  2.2589782e-08,  0.0000000e+00,\n",
       "           7.7343500e-09],\n",
       "         [ 0.0000000e+00, -2.1507851e-08, -2.2620128e-08,  0.0000000e+00,\n",
       "          -7.7447391e-09],\n",
       "         [ 0.0000000e+00,  4.8431625e-10,  5.0936266e-10,  0.0000000e+00,\n",
       "           1.7439694e-10]], dtype=float32)>,\n",
       "  <tf.Tensor: shape=(5,), dtype=float32, numpy=\n",
       "  array([0.0000000e+00, 1.7866494e-08, 1.8790459e-08, 0.0000000e+00,\n",
       "         6.4335270e-09], dtype=float32)>]]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense = tf.keras.layers.Dense(5, activation='relu')\n",
    "b = tf.Variable(tf.zeros((1,1), dtype=tf.float32), name='b', trainable=True)\n",
    "#b = tf.tile(b, [100, 1])\n",
    "\n",
    "grads = \n",
    "x = tf.random.normal((1,3))\n",
    "with tf.GradientTape() as tape:\n",
    "    y = dense(x)\n",
    "    y = tf.concat((b, y), axis = -1)\n",
    "    y = tf.keras.layers.Activation('softmax')(y)\n",
    "\n",
    "print([var.name for var in tape.watched_variables()])\n",
    "grads = tape.gradient(y, [b, dense.trainable_variables])\n",
    "grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dense_3/kernel:0', 'dense_3/bias:0', 'b:0']\n"
     ]
    }
   ],
   "source": [
    "batch_size = 10\n",
    "inputs = tf.keras.Input(shape = (3,), batch_size=batch_size)\n",
    "x = tf.keras.layers.Dense(5, activation='relu')(inputs)\n",
    "b = tf.Variable(tf.zeros((batch_size, 1), dtype=tf.float32), name='b', trainable=True)\n",
    "#b = tf.tile(b, [x.shape[0], 1])\n",
    "x = tf.concat((b, x), axis = 1)\n",
    "\n",
    "outputs = tf.keras.layers.Activation('softmax')(x)\n",
    "model = tf.keras.Model(inputs = inputs, outputs = outputs)\n",
    "\n",
    "x = np.random.rand(batch_size, 3)\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(b)\n",
    "    y = model(x)\n",
    "\n",
    "print([var.name for var in tape.watched_variables()])\n",
    "grads = tape.gradient(y, [b, model.trainable_variables])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " [<tf.Tensor: shape=(3, 5), dtype=float32, numpy=\n",
       "  array([[ 0.0000000e+00, -4.2425383e-09, -2.4098763e-09, -3.7232202e-09,\n",
       "          -5.4903815e-09],\n",
       "         [ 0.0000000e+00, -8.1804163e-09, -7.8085822e-09, -6.5687202e-09,\n",
       "          -1.6969963e-08],\n",
       "         [ 0.0000000e+00, -4.4441677e-09,  8.2542240e-10, -2.7874085e-09,\n",
       "          -9.4563646e-10]], dtype=float32)>,\n",
       "  <tf.Tensor: shape=(5,), dtype=float32, numpy=\n",
       "  array([ 0.0000000e+00, -6.6218666e-09, -1.9416655e-09, -4.6989950e-09,\n",
       "         -6.5238552e-09], dtype=float32)>]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.trainable_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(784,), name=\"digits\")\n",
    "x1 = layers.Dense(64, activation=\"relu\")(inputs)\n",
    "x2 = layers.Dense(64, activation=\"relu\")(x1)\n",
    "outputs = layers.Dense(10, name=\"predictions\")(x2)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate an optimizer.\n",
    "optimizer = keras.optimizers.SGD(learning_rate=1e-3)\n",
    "# Instantiate a loss function.\n",
    "loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the training dataset.\n",
    "batch_size = 64\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "x_train = np.reshape(x_train, (-1, 784))\n",
    "x_test = np.reshape(x_train, (-1, 784))\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TensorShape([784, 64]), TensorShape([64]), TensorShape([64, 64]), TensorShape([64]), TensorShape([64, 10]), TensorShape([10])]\n",
      "[<tf.Variable 'dense_2/kernel:0' shape=(784, 64) dtype=float32, numpy=\n",
      "array([[ 0.07894983,  0.03262346,  0.00714903, ...,  0.06350061,\n",
      "        -0.05786685, -0.06261454],\n",
      "       [ 0.00303822, -0.018301  , -0.07591124, ..., -0.08166868,\n",
      "        -0.05336305,  0.04615601],\n",
      "       [-0.06759747, -0.05747033, -0.04010406, ..., -0.03093509,\n",
      "         0.07951189,  0.05656417],\n",
      "       ...,\n",
      "       [ 0.01547523,  0.06662116,  0.01748057, ...,  0.0673169 ,\n",
      "        -0.04742297, -0.0036445 ],\n",
      "       [-0.01280259,  0.07242832,  0.03060477, ...,  0.06895592,\n",
      "         0.05302786,  0.02464844],\n",
      "       [-0.03255052,  0.00413933,  0.03820144, ...,  0.06641275,\n",
      "         0.06434075, -0.00919166]], dtype=float32)>, <tf.Variable 'dense_2/bias:0' shape=(64,) dtype=float32, numpy=\n",
      "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>, <tf.Variable 'dense_3/kernel:0' shape=(64, 64) dtype=float32, numpy=\n",
      "array([[ 0.11544584,  0.11577727, -0.17490076, ..., -0.01797022,\n",
      "         0.21494962, -0.01585013],\n",
      "       [ 0.06393148, -0.09419429, -0.21429086, ...,  0.06704898,\n",
      "        -0.20372498, -0.02771989],\n",
      "       [ 0.21255161, -0.03643128, -0.05441885, ..., -0.19116455,\n",
      "        -0.11163459, -0.02280296],\n",
      "       ...,\n",
      "       [ 0.19606103,  0.15668519,  0.2141747 , ..., -0.11814577,\n",
      "         0.19131865,  0.14589368],\n",
      "       [ 0.11578368, -0.05536291,  0.19454025, ...,  0.1340565 ,\n",
      "        -0.19893122, -0.08589996],\n",
      "       [ 0.06184568, -0.07956521, -0.18332422, ..., -0.04824144,\n",
      "         0.00368452, -0.17804855]], dtype=float32)>, <tf.Variable 'dense_3/bias:0' shape=(64,) dtype=float32, numpy=\n",
      "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>, <tf.Variable 'predictions/kernel:0' shape=(64, 10) dtype=float32, numpy=\n",
      "array([[-1.40250191e-01, -1.55158222e-01, -4.40239608e-02,\n",
      "        -2.34932497e-01,  2.58227617e-01,  3.19739282e-02,\n",
      "         1.87500119e-02,  1.79666936e-01, -1.97356313e-01,\n",
      "         9.93972719e-02],\n",
      "       [ 5.56589961e-02, -7.76875019e-03, -4.68915254e-02,\n",
      "        -8.00173134e-02,  2.07467645e-01, -2.60466188e-01,\n",
      "         2.82125086e-01,  5.55745959e-02, -1.63364172e-01,\n",
      "        -2.83587307e-01],\n",
      "       [-1.77737445e-01, -9.67339724e-02, -2.12520659e-02,\n",
      "        -2.04858392e-01, -2.13760346e-01, -7.03188777e-03,\n",
      "        -1.59588799e-01,  1.29931122e-01,  2.75357455e-01,\n",
      "        -2.28735209e-02],\n",
      "       [ 1.38545930e-02, -2.76103407e-01,  2.34346390e-02,\n",
      "         1.00750625e-01,  1.26711071e-01,  1.72685474e-01,\n",
      "        -1.25126824e-01, -2.75343060e-02, -1.11875072e-01,\n",
      "         1.67901129e-01],\n",
      "       [ 7.70805776e-02, -4.78685796e-02,  9.58795249e-02,\n",
      "        -1.11770317e-01,  1.46001220e-01,  2.66449243e-01,\n",
      "        -1.99554682e-01,  1.27162933e-02, -2.67232478e-01,\n",
      "        -1.57106712e-01],\n",
      "       [-1.23621225e-02,  6.51297867e-02,  2.64243811e-01,\n",
      "        -1.17606342e-01,  7.23688006e-02,  1.87723398e-01,\n",
      "        -5.17652184e-02, -9.03779268e-02, -3.15795541e-02,\n",
      "         2.09888071e-01],\n",
      "       [ 2.17214972e-01, -2.24914178e-01, -5.34678698e-02,\n",
      "         1.14946097e-01,  2.18438417e-01, -1.63557738e-01,\n",
      "        -1.21318653e-01, -1.26693636e-01, -1.78571537e-01,\n",
      "        -1.86598599e-01],\n",
      "       [-2.24444658e-01,  7.73400068e-03,  2.01343626e-01,\n",
      "         2.30089754e-01,  1.13992006e-01, -1.47907138e-02,\n",
      "        -4.95052487e-02,  5.62116206e-02,  7.66434371e-02,\n",
      "         3.54560912e-02],\n",
      "       [ 1.87482715e-01, -9.78031754e-03, -1.79732502e-01,\n",
      "        -1.75874576e-01, -2.71296799e-02,  2.52500266e-01,\n",
      "         2.39893794e-03, -1.29862562e-01, -2.32781917e-01,\n",
      "         4.65863645e-02],\n",
      "       [ 2.83033937e-01,  6.43728077e-02,  2.44549602e-01,\n",
      "         1.07355773e-01,  2.25167900e-01, -2.18390048e-01,\n",
      "        -7.11027980e-02,  2.42206663e-01,  3.24291885e-02,\n",
      "        -2.77406871e-01],\n",
      "       [-1.32633850e-01,  2.53817439e-02,  2.40417093e-01,\n",
      "        -8.70088637e-02,  2.50244588e-01, -2.78354496e-01,\n",
      "         1.62262559e-01, -3.31699848e-03,  2.25202113e-01,\n",
      "        -1.28265738e-01],\n",
      "       [-5.15266508e-02,  2.62295336e-01, -1.74768731e-01,\n",
      "        -1.41958550e-01,  1.00023806e-01,  1.55473769e-01,\n",
      "         3.22656929e-02, -9.68136787e-02,  2.02118993e-01,\n",
      "         2.19898492e-01],\n",
      "       [ 8.61173570e-02,  1.17600352e-01, -2.67018437e-01,\n",
      "        -8.49705040e-02, -2.25494966e-01, -3.10303867e-02,\n",
      "         2.21015960e-01, -1.49700209e-01, -7.41587579e-02,\n",
      "         2.72406191e-01],\n",
      "       [-2.10596621e-02,  1.77110642e-01,  7.58978724e-02,\n",
      "        -8.93049538e-02, -2.21499562e-01,  1.28693223e-01,\n",
      "        -2.00494409e-01,  6.16385341e-02, -1.58134624e-01,\n",
      "        -1.05888277e-01],\n",
      "       [ 3.89882922e-02,  2.54331917e-01, -2.70205140e-02,\n",
      "        -2.19813287e-02,  2.21774727e-01,  1.78655863e-01,\n",
      "         1.58560157e-01, -2.80513227e-01, -5.62375337e-02,\n",
      "        -2.21036345e-01],\n",
      "       [-2.10241377e-01, -1.84145704e-01,  3.53851318e-02,\n",
      "         2.66980320e-01, -7.15015829e-02, -2.81509012e-01,\n",
      "         1.15722150e-01,  1.77259922e-01,  1.76573366e-01,\n",
      "         3.46057713e-02],\n",
      "       [ 3.81787121e-02,  1.06305122e-01,  1.17844284e-01,\n",
      "        -2.67904788e-01,  2.37384409e-01,  3.40581238e-02,\n",
      "        -6.29704446e-02,  2.69200653e-01, -2.17812434e-01,\n",
      "        -1.29549861e-01],\n",
      "       [ 1.42773688e-01, -7.56049305e-02, -2.60823488e-01,\n",
      "         2.34068662e-01, -1.01830751e-01, -7.66813755e-04,\n",
      "        -2.84448802e-01, -2.49648094e-03,  2.00369686e-01,\n",
      "         8.07947814e-02],\n",
      "       [ 2.48810440e-01, -5.54945618e-02, -1.84316784e-01,\n",
      "        -3.21540833e-02, -1.02225319e-01,  1.62047327e-01,\n",
      "         1.98775232e-02,  2.57476360e-01, -3.48370671e-02,\n",
      "        -3.56336087e-02],\n",
      "       [-2.44618654e-02,  1.49702668e-01, -2.61255503e-02,\n",
      "        -5.25474250e-02,  2.80521244e-01, -1.71355546e-01,\n",
      "        -2.18451738e-01, -4.90446985e-02, -1.40587673e-01,\n",
      "         1.46640331e-01],\n",
      "       [-1.56239420e-01, -2.84512222e-01, -1.20687485e-01,\n",
      "        -2.37116218e-03, -2.13317230e-01,  2.23805219e-01,\n",
      "        -2.33510494e-01, -2.39195317e-01,  1.14565641e-01,\n",
      "        -2.38694161e-01],\n",
      "       [-8.01539421e-03,  1.48936927e-01, -7.75821954e-02,\n",
      "        -2.59894490e-01,  2.02978462e-01, -6.18166625e-02,\n",
      "         4.48634624e-02, -2.24096313e-01,  9.57255661e-02,\n",
      "        -2.08964735e-01],\n",
      "       [ 2.29481459e-02, -2.36139163e-01,  1.74451679e-01,\n",
      "        -8.80030394e-02, -1.02317855e-01,  2.16142088e-01,\n",
      "         9.99375284e-02,  1.75462633e-01, -2.09034309e-01,\n",
      "         2.92169452e-02],\n",
      "       [-2.58143574e-01, -7.45591670e-02,  7.70555735e-02,\n",
      "        -2.84513026e-01,  1.34456009e-01,  3.97967696e-02,\n",
      "         2.60844827e-03, -6.80096447e-02,  7.77244866e-02,\n",
      "         1.07375443e-01],\n",
      "       [ 2.75922209e-01,  2.14313090e-01, -1.51732072e-01,\n",
      "        -2.02421099e-01,  2.05114484e-03,  2.54158288e-01,\n",
      "         1.43590540e-01, -1.79509759e-01,  1.41017139e-02,\n",
      "        -2.08554804e-01],\n",
      "       [ 1.26346499e-01, -1.99058965e-01,  2.68961817e-01,\n",
      "        -9.78142917e-02,  1.90377861e-01,  1.61835402e-01,\n",
      "         8.65932405e-02,  2.70903319e-01,  2.72659928e-01,\n",
      "         2.03248113e-01],\n",
      "       [-1.87178984e-01,  1.30921751e-01, -2.84195036e-01,\n",
      "         2.08159298e-01,  2.18853801e-01, -2.45125711e-01,\n",
      "         1.87206626e-01,  2.33656615e-01, -7.84925222e-02,\n",
      "         7.78985023e-02],\n",
      "       [-2.62706131e-01,  2.40424901e-01, -1.90951437e-01,\n",
      "        -3.37694585e-02,  1.21506214e-01,  1.18873894e-01,\n",
      "        -2.72963881e-01, -6.78037256e-02,  1.43895507e-01,\n",
      "         5.73985875e-02],\n",
      "       [ 6.69534206e-02, -3.22082639e-02, -2.52990246e-01,\n",
      "         1.82897836e-01,  6.88974857e-02,  2.72306502e-02,\n",
      "         9.83502269e-02,  2.60522366e-02,  1.50370091e-01,\n",
      "         1.76928699e-01],\n",
      "       [-9.78009850e-02, -1.48609444e-01,  1.79522872e-01,\n",
      "        -5.95683753e-02, -1.14935100e-01,  4.43258584e-02,\n",
      "         8.18389654e-03,  7.66840875e-02,  2.21758336e-01,\n",
      "         2.19846338e-01],\n",
      "       [-1.29498199e-01,  2.27016956e-01, -2.73880124e-01,\n",
      "         1.79754496e-01, -1.84796154e-01, -1.11785457e-01,\n",
      "        -2.43397519e-01, -6.87966049e-02, -1.87862694e-01,\n",
      "         1.92185670e-01],\n",
      "       [ 1.98268592e-02,  2.54060000e-01, -1.72420442e-01,\n",
      "         6.25820458e-02, -1.83577403e-01,  2.12825507e-01,\n",
      "         1.77264214e-04, -6.17597103e-02, -1.51877999e-02,\n",
      "         8.21802020e-02],\n",
      "       [-1.33688450e-01,  4.00117040e-03, -4.33512479e-02,\n",
      "        -1.42010152e-01,  2.41928548e-01,  2.03836739e-01,\n",
      "         1.20481849e-03,  6.32901192e-02, -1.92031503e-01,\n",
      "         2.05502987e-01],\n",
      "       [-1.99848413e-02, -2.14201421e-01, -1.90109015e-03,\n",
      "        -2.81002849e-01, -9.32624936e-03,  1.11720085e-01,\n",
      "        -1.34893537e-01, -6.51077181e-02,  1.84222162e-01,\n",
      "        -1.08458087e-01],\n",
      "       [-2.80990154e-01,  1.23970747e-01,  2.66672701e-01,\n",
      "         6.94828331e-02,  1.25156343e-01,  8.02138448e-02,\n",
      "        -2.12801695e-02,  1.03293628e-01,  2.44143158e-01,\n",
      "        -2.33270511e-01],\n",
      "       [ 8.98748040e-02,  1.27042383e-01, -2.03296542e-01,\n",
      "         2.71379650e-02, -3.37167084e-02, -6.73651695e-02,\n",
      "         2.62900621e-01, -3.02101672e-02, -6.60430938e-02,\n",
      "         6.79798424e-02],\n",
      "       [-1.91218928e-01,  2.15675741e-01, -5.70889413e-02,\n",
      "         2.84044474e-01,  1.07804030e-01, -1.83970630e-02,\n",
      "         1.88256055e-01,  2.72260517e-01, -1.49973631e-02,\n",
      "         4.30189371e-02],\n",
      "       [ 8.61848295e-02, -1.33799106e-01,  1.59194440e-01,\n",
      "        -1.90398365e-01,  3.57715786e-02,  1.01577729e-01,\n",
      "        -1.49481341e-01, -1.61778703e-01,  1.18311644e-01,\n",
      "         1.96256906e-01],\n",
      "       [ 1.76271588e-01, -2.78121084e-01, -1.98420882e-02,\n",
      "        -7.86146522e-02, -2.81169176e-01, -1.59046978e-01,\n",
      "         4.33839560e-02, -1.83993369e-01, -2.24279210e-01,\n",
      "         2.28049546e-01],\n",
      "       [ 1.97133154e-01, -2.58250952e-01, -2.00238615e-01,\n",
      "         2.74505824e-01,  2.68400639e-01,  1.15005434e-01,\n",
      "         5.34957647e-02, -2.62746245e-01,  8.70302022e-02,\n",
      "         1.68012679e-02],\n",
      "       [ 2.72003204e-01,  5.99588156e-02, -7.87346810e-02,\n",
      "         7.65788555e-02,  9.73887146e-02,  1.73871517e-02,\n",
      "        -2.31821626e-01, -4.51085567e-02,  2.05172092e-01,\n",
      "         7.64821172e-02],\n",
      "       [ 2.28748828e-01,  1.61541492e-01, -1.33767053e-01,\n",
      "         9.59410965e-02,  2.62485832e-01, -1.90295234e-01,\n",
      "         1.96269751e-01, -7.74232000e-02, -8.48451257e-03,\n",
      "         1.42815650e-01],\n",
      "       [ 1.30237639e-01, -2.07088619e-01, -5.38712591e-02,\n",
      "        -1.17482439e-01, -1.38463482e-01,  1.61784291e-02,\n",
      "        -2.39456281e-01, -5.04176766e-02, -8.09869021e-02,\n",
      "        -1.68761492e-01],\n",
      "       [ 2.42198914e-01, -1.50525272e-01, -8.63278806e-02,\n",
      "        -5.29855192e-02,  2.09993392e-01,  2.05250591e-01,\n",
      "        -1.39992893e-01, -2.60557979e-01,  1.74388409e-01,\n",
      "        -1.01623014e-01],\n",
      "       [-2.51661241e-02,  2.29133934e-01, -6.13361448e-02,\n",
      "        -5.57303429e-05,  1.62792563e-01,  2.50269026e-01,\n",
      "        -2.41219223e-01,  2.63895184e-01, -1.77100778e-01,\n",
      "        -1.05749369e-02],\n",
      "       [ 9.42373574e-02, -7.45631754e-02, -1.17812321e-01,\n",
      "         2.42706805e-01, -3.64352465e-02,  1.09265000e-01,\n",
      "         2.03937411e-01, -7.50507563e-02, -2.08892629e-01,\n",
      "         1.67897344e-01],\n",
      "       [-1.06392488e-01, -3.53201032e-02,  1.99781448e-01,\n",
      "        -2.54891396e-01, -1.70716912e-01,  3.50362659e-02,\n",
      "         1.42079115e-01,  1.23508692e-01, -1.23715818e-01,\n",
      "        -2.42033422e-01],\n",
      "       [ 1.61733091e-01,  2.12613285e-01,  2.05556482e-01,\n",
      "        -2.36696526e-01,  1.40142918e-01, -1.25035241e-01,\n",
      "        -2.17780322e-01,  5.95164299e-03,  1.11900657e-01,\n",
      "         4.26893234e-02],\n",
      "       [-6.01439476e-02, -1.10824287e-01, -1.41068116e-01,\n",
      "         2.22880572e-01,  1.26467615e-01,  3.29337418e-02,\n",
      "        -1.33289188e-01,  1.40672803e-01,  1.63338780e-01,\n",
      "         1.46349490e-01],\n",
      "       [-1.13660619e-01, -1.72612906e-01,  4.92281318e-02,\n",
      "         2.73618102e-02, -1.08875737e-01, -1.20738879e-01,\n",
      "        -2.84252763e-01,  2.02640533e-01, -1.20229423e-02,\n",
      "         2.31348485e-01],\n",
      "       [-2.39184454e-01, -3.05999815e-02, -2.14727283e-01,\n",
      "        -1.40604988e-01,  6.09891713e-02, -2.48640448e-01,\n",
      "        -1.13623083e-01,  4.99797165e-02,  1.81954980e-01,\n",
      "         2.47362942e-01],\n",
      "       [ 6.31060898e-02,  1.65760189e-01, -2.39640668e-01,\n",
      "        -6.05649203e-02, -1.70121461e-01,  1.46534473e-01,\n",
      "         1.16704971e-01,  1.87323868e-01, -1.74167037e-01,\n",
      "         8.52956176e-02],\n",
      "       [ 2.79953212e-01,  2.32828945e-01,  8.65854323e-02,\n",
      "        -2.66117066e-01, -2.15387642e-01, -8.46081972e-03,\n",
      "        -4.85066772e-02, -5.18043786e-02, -3.27089429e-02,\n",
      "        -2.55426764e-03],\n",
      "       [-1.39245704e-01, -6.05816245e-02,  1.75117344e-01,\n",
      "         8.27523470e-02, -2.07857519e-01,  2.12443978e-01,\n",
      "        -2.03700200e-01, -7.53954202e-02,  1.29298270e-01,\n",
      "         2.73890048e-01],\n",
      "       [-2.52472937e-01,  2.48084456e-01, -2.61062801e-01,\n",
      "        -2.69226044e-01, -2.11289316e-01,  1.27509177e-01,\n",
      "         2.69093424e-01,  2.68818110e-01,  1.67533040e-01,\n",
      "        -9.78764892e-03],\n",
      "       [-1.05782509e-01, -2.33366489e-02,  2.36448944e-02,\n",
      "        -7.93446749e-02, -1.43173292e-01,  6.94834292e-02,\n",
      "         9.32848752e-02,  2.18307465e-01,  1.74331784e-01,\n",
      "         1.42273903e-01],\n",
      "       [ 1.65182650e-01,  1.10685378e-01, -1.67120814e-01,\n",
      "        -1.16150931e-01,  1.52443558e-01,  2.42051274e-01,\n",
      "        -1.91055447e-01, -1.76225156e-01,  2.15961367e-01,\n",
      "        -8.43909383e-02],\n",
      "       [-2.27783233e-01, -5.73853403e-02, -2.77338058e-01,\n",
      "         2.00177312e-01,  2.00262785e-01,  2.04558253e-01,\n",
      "         1.82704270e-01,  7.82212317e-02,  1.77826941e-01,\n",
      "        -1.87139481e-01],\n",
      "       [ 7.12341666e-02,  1.72775179e-01, -7.32831955e-02,\n",
      "         1.70212090e-01, -1.37721524e-01,  3.43587995e-03,\n",
      "        -1.64624751e-02, -2.30083451e-01, -2.02812016e-01,\n",
      "        -1.96058005e-01],\n",
      "       [ 2.52659053e-01, -1.41463771e-01, -6.11515492e-02,\n",
      "        -1.84659690e-01,  1.81318194e-01, -1.48737475e-01,\n",
      "        -2.63426363e-01,  2.55437523e-01, -1.60461366e-02,\n",
      "         1.17451698e-01],\n",
      "       [ 2.30199426e-01,  2.37217516e-01,  9.48495865e-02,\n",
      "         2.12655246e-01, -8.66651535e-03, -2.56573021e-01,\n",
      "        -2.73371041e-02, -1.82943195e-01,  3.07160020e-02,\n",
      "        -2.20653415e-03],\n",
      "       [ 2.59088784e-01,  1.18514895e-01,  2.41790086e-01,\n",
      "        -5.70920110e-03, -5.02728671e-02, -8.49524140e-03,\n",
      "        -2.51652896e-01,  3.17708552e-02, -2.24372566e-01,\n",
      "         9.77888405e-02],\n",
      "       [ 2.22473174e-01,  2.62680143e-01,  1.59942359e-01,\n",
      "        -2.51377523e-01,  2.90336311e-02, -2.02842489e-01,\n",
      "        -1.48057431e-01, -8.86992365e-02, -1.16095066e-01,\n",
      "         2.68069357e-01],\n",
      "       [ 8.19949210e-02,  2.29544193e-01, -9.07127559e-02,\n",
      "        -2.13193804e-01, -1.46903723e-01,  1.07923150e-03,\n",
      "        -2.73376316e-01,  2.49869615e-01, -2.81641185e-02,\n",
      "        -1.32936507e-01]], dtype=float32)>, <tf.Variable 'predictions/bias:0' shape=(10,) dtype=float32, numpy=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "for (x, y) in train_dataset:\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "\n",
    "        # Run the forward pass of the layer.\n",
    "        # The operations that the layer applies\n",
    "        # to its inputs are going to be recorded\n",
    "        # on the GradientTape.\n",
    "        logits = model(x, training=True)  # Logits for this minibatch\n",
    "\n",
    "        # Compute the loss value for this minibatch.\n",
    "        loss_value = loss_fn(y, logits)\n",
    "\n",
    "    # Use the gradient tape to automatically retrieve\n",
    "    # the gradients of the trainable variables with respect to the loss.\n",
    "    grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "\n",
    "    # Run one step of gradient descent by updating\n",
    "    # the value of the variables to minimize the loss.\n",
    "    # optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "    shapes = [grad.shape for grad in grads]\n",
    "    print(shapes)\n",
    "    print(model.trainable_variables)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
