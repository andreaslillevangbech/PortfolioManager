{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.MultiIndex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer cash_6 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "['Variable:0', 'cash_6/dense_6/kernel:0', 'cash_6/dense_6/bias:0']\n"
     ]
    }
   ],
   "source": [
    "class Cash(tf.keras.Model):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Cash, self).__init__()\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(5, activation='relu')\n",
    " \n",
    "        self.b = tf.Variable(tf.zeros((1, 1), dtype=tf.float32), trainable=True)\n",
    "        self.out = tf.keras.layers.Activation('softmax')\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.dense(x)\n",
    "        bias = tf.tile(self.b, [tf.shape(x)[0], 1])\n",
    "        x = tf.concat((bias, x), axis = -1)\n",
    "        x = self.out(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    \n",
    "model = Cash()\n",
    "\n",
    "X = np.random.randn(100, 3)\n",
    "with tf.GradientTape() as tape:\n",
    "    y = model(X)\n",
    "\n",
    "print([var.name for var in tape.watched_variables()])\n",
    "grads = tape.gradient(y, model.trainable_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer cash_8 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "(100, 11)\n",
      "['conv1_1/kernel:0', 'conv1_1/bias:0', 'conv2_1/kernel:0', 'conv2_1/bias:0', 'Variable:0']\n"
     ]
    }
   ],
   "source": [
    "class Cash(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, rows = 11, cols = 50, features = 3):\n",
    "        super(Cash, self).__init__()\n",
    "        \n",
    "        input_shape = (rows, cols, features)\n",
    "        inputs = tf.keras.Input(shape=input_shape)\n",
    "        x = tf.keras.layers.Conv2D(\n",
    "            filters = 2, \n",
    "            kernel_size = (1,3), \n",
    "            padding='valid', \n",
    "            activation='relu',\n",
    "            name = 'conv1'\n",
    "        )(inputs)\n",
    "        x = tf.keras.layers.Conv2D(1, \n",
    "                                (1, x.shape[2]), \n",
    "                                activation=\"relu\", \n",
    "                                name = 'conv2')(x)\n",
    "        x = tf.squeeze(x)\n",
    "        \n",
    "        self.model = tf.keras.Model(inputs = inputs, outputs = x)\n",
    "        \n",
    "        self.b = tf.Variable(tf.zeros((1, 1), dtype=tf.float32), trainable=True)\n",
    "        self.out = tf.keras.layers.Activation('softmax')\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.model(x)\n",
    "        print(x.shape)\n",
    "        bias = tf.tile(self.b, [tf.shape(x)[0], 1])\n",
    "        x = tf.concat((bias, x), axis = -1)\n",
    "        x = self.out(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    \n",
    "model = Cash()\n",
    "\n",
    "X = np.random.randn(100, 11, 50, 3)\n",
    "with tf.GradientTape() as tape:\n",
    "    y = model(X)\n",
    "\n",
    "print([var.name for var in tape.watched_variables()])\n",
    "grads = tape.gradient(y, model.trainable_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = 100\n",
    "dense = tf.keras.layers.Dense(5, activation='relu')\n",
    "b = tf.Variable(tf.zeros((1,1), dtype=tf.float32), name='b', trainable=True)\n",
    "\n",
    "x =  tf.random.normal((batch, 3))\n",
    "with tf.GradientTape() as tape:\n",
    "    y = dense(x)\n",
    "    cash = tf.tile(b, [y.shape[0], 1])\n",
    "    y = tf.concat((cash, y), axis = -1)\n",
    "    y = tf.keras.layers.Activation('softmax')(y)\n",
    "\n",
    "print([var.name for var in tape.watched_variables()])\n",
    "grads = tape.gradient(y, [b, dense.trainable_variables])\n",
    "grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN(rows, cols, features, batch_size):\n",
    "    input_shape = (rows, cols, features)\n",
    "    X = keras.Input(shape= input_shape, batch_size=batch_size)\n",
    "    w = keras.Input(shape = (rows, 1, 1), batch_size=batch_size)\n",
    "\n",
    "    x = tf.keras.layers.Conv2D(\n",
    "        filters = 2, \n",
    "        kernel_size = (1,3), \n",
    "        padding='valid', \n",
    "        activation='relu'\n",
    "    )(X)\n",
    "        \n",
    "    x = keras.layers.Conv2D(20, \n",
    "                            (1, x.shape[2]), \n",
    "                            activation=\"relu\", \n",
    "                            name = 'conv2')(x)  \n",
    "    \n",
    "    con = keras.layers.Concatenate(axis=3)([x, w])\n",
    "\n",
    "    x = keras.layers.Conv2D(1, (1,1), name = 'votes')(con)\n",
    "    x = tf.squeeze(x)\n",
    "    \n",
    "    b = tf.tile(b, [x.shape[0], 1])\n",
    "    with_bias = keras.layers.Concatenate(axis=1)([b, x])\n",
    "    #with_bias = CashBias()(x)\n",
    "\n",
    "    outputs = keras.layers.Activation('softmax')(with_bias)\n",
    "    return keras.Model(inputs = [X, w], outputs = outputs, name = \"Policy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(784,), name=\"digits\")\n",
    "x1 = layers.Dense(64, activation=\"relu\")(inputs)\n",
    "x2 = layers.Dense(64, activation=\"relu\")(x1)\n",
    "outputs = layers.Dense(10, name=\"predictions\")(x2)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate an optimizer.\n",
    "optimizer = keras.optimizers.SGD(learning_rate=1e-3)\n",
    "# Instantiate a loss function.\n",
    "loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the training dataset.\n",
    "batch_size = 64\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "x_train = np.reshape(x_train, (-1, 784))\n",
    "x_test = np.reshape(x_train, (-1, 784))\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (x, y) in train_dataset:\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "\n",
    "        # Run the forward pass of the layer.\n",
    "        # The operations that the layer applies\n",
    "        # to its inputs are going to be recorded\n",
    "        # on the GradientTape.\n",
    "        logits = model(x, training=True)  # Logits for this minibatch\n",
    "\n",
    "        # Compute the loss value for this minibatch.\n",
    "        loss_value = loss_fn(y, logits)\n",
    "\n",
    "    # Use the gradient tape to automatically retrieve\n",
    "    # the gradients of the trainable variables with respect to the loss.\n",
    "    grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "\n",
    "    # Run one step of gradient descent by updating\n",
    "    # the value of the variables to minimize the loss.\n",
    "    # optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "    shapes = [grad.shape for grad in grads]\n",
    "    print(shapes)\n",
    "    print(model.trainable_variables)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [np.random.rand(2,3) for i in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float32')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(a, dtype='float32').dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
